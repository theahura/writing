<title>SimpleDL Part 5: Canonical Tasks</title>
<meta name="viewport" content="width=800" />
<link rel="icon" href="spy.ico" type="image/x-icon" />
<link rel="shortcut icon" href="spy.ico" type="image/x-icon" />
<link rel="stylesheet" href="theme.css" />

<!-- Google Analytics -->
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-131666667-1"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "UA-131666667-1");
</script>

<div class="header">
  <h1>Writing</h1>
  <h3>Amol Kapoor</h3>
</div>

<div class="content">
  <div class="writing-holder">
    <h4>Simple DL Part 5: Canonical Tasks</h4>
    <h6>February, 2021</h6>

    <div class="writing">
      <h6>TLDR</h6>
      <ul>
        <li>TODO</li>
      </ul>

      <h6>What is a Canonical Task?</h6>
      <p>
        There is a lot of chaos in deep learning literature. Every year, it
        seems like hundreds of models are published, each with dozens of
        possible applications. It can be tough to figure out what a model
        actually does and how it can be used.
      </p>
      <p>
        Over time, I developed this idea of a Canonical Task to help me get
        through the noise. For me, a Canonical Task is some sort of indivisible
        thing that defines a models purpose. Depending on what's more intuitive,
        you can think of these tasks based on what the model produces, or based
        on what the model does. It turns out that even though there are so many
        different models, there are only three canonical tasks. These are:
      </p>
      <ul>
        <li>Classification</li>
        <li>Multi-Classification</li>
        <li>Regression</li>
      </ul>
      <p>
        Every deep learning model in the whole world can be broken down into
        these three tasks. And each of these tasks comes with a set of 'default'
        loss functions too. So if you have a never-before-solved problem, the
        first thing to do is figure out which task your problem falls under, and
        then pick a corresponding loss, and you've already solved half the issue
        in front of you.
      </p>
      <p>
        Let's talk about what each of these Canonical Tasks mean in a bit more
        detail.
      </p>

      <p>
        A <b>classification</b> task is when you train a model to predict
        whether an input is one of a set of mutually exclusive things. Imagine
        we want to train a pet classifier. We want to pass the model some input,
        maybe an image. On the output, the model needs to decide whether the
        input represented a dog, a cat, or a fish. The model cannot say the
        input was a dog AND a cat AND a fish -- these are mutually exclusive
        categories. If the model is more confident that the input is a cat, it
        has to be less confident that the input is a dog or a fish.
      </p>
      <p>
        A really common loss for classification tasks is softmax cross entropy
        loss. This loss works best if you can represent groundtruth labels as a
        one-hot vector representing the label. We use a softmax nonlinearity to
        reweight the model outputs because softmax adjusts every value in the
        vector relative to the others such that the total sums to one. Since the
        output vector sums to one, we can think of it as a probability
        distribution. The easiest way to interpret the model output with this
        loss is to examine the weight that is highest, and call that the model's
        prediction. If many values are close, we can say the model is not so
        confident.
      </p>
      <p>
        A <b>multi-classification</b> task is similar to a classification task,
        except this time the possible outputs are not mutually exclusive. Maybe
        we have a dataset of zoo animals, and so it's possible for an image to
        contain cats and dogs and fish. We would use a multi-classification
        model to represent this data.
      </p>
      <p>
        A pretty common multi-classification loss is sigmoid cross entropy. This
        loss is similar to the softmax cross entropy, but we use a sigmoid
        nonlinearity because that is applied per element and simply converts
        each individual element to a value between 0 and 1. Because the
        individual elements aren't constrained by each other, multiple elements
        can be set to 1 (i.e. 'predicted'). One way to think about this model
        output is to imagine that each individual element is its own predictive
        function, and any value above a threshold (commonly 0.5) is therefore
        considered a model prediction.
      </p>

      <p>
        A <b>regression</b> task is when you train a model to predict continuous
        values. The other two types of tasks predict categories that don't have
        any directional relation to each other. By comparison, regression tasks
        produce outputs where predicting a higher or lower value matters because
        of the numerical relationship between those values. For example, if we
        wanted to predict housing prices, or case counts for some disease, or
        some embedding value from a different model, we want to use regression.
      </p>
      <p>
        There are a bunch of possible losses here, such as difference, mean
        squared error or mean squared log error. There are theoretical reasons
        to prefer a log error, but basically as long as you have some measure of
        distance between the ground truth and the model prediction as your loss,
        you are probably good.
      </p>

      <h6>Outputs vs. Tasks</h6>
      <p>
        The tasks above aren't necessarily the same thing as the model output.
        These tasks basically define ways for us to think about and categorize
        models (or subsets of models). The actual outputs can be anything within
        the model. For example, we may want to train a model with a
        multiclassification loss on images, in order to pull out the top layer
        of the model as an image embedding. For your own intuition, it's
        important to avoid confusing these two concepts (even though they are
        fairly similar in many learning tasks).
      </p>
    </div>
  </div>
  <div class="footer">
    <a href="./index.html">Back to Writing</a>
  </div>
</div>
