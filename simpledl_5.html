<title>SimpleDL Part 5: Canonical Tasks</title>
<meta name="viewport" content="width=800" />
<link rel="icon" href="spy.ico" type="image/x-icon" />
<link rel="shortcut icon" href="spy.ico" type="image/x-icon" />
<link rel="stylesheet" href="theme.css" />

<!-- Google Analytics -->
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-131666667-1"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "UA-131666667-1");
</script>

<div class="header">
  <h1>Writing</h1>
  <h3>Amol Kapoor</h3>
</div>

<div class="content">
  <div class="writing-holder">
    <h4>Simple DL Part 5: Canonical Tasks</h4>
    <h6>February, 2021</h6>

    <div class="writing">
      <h6>TLDR</h6>
      <ul>
        <li>TODO</li>
      </ul>

      <h6>What is a Canonical Task?</h6>
      <p>
        There is a lot of chaos in deep learning literature. Every year, it
        seems like hundreds of models are published, each with dozens of
        possible applications. It can be tough to figure out what a model
        actually does and how it can be used.
      </p>
      <p>
        Over time, I developed this idea of a Canonical Task to help me get
        through the noise. For me, a Canonical Task is some sort of indivisible
        thing that defines a models purpose. Depending on what's more intuitive,
        you can think of these tasks based on what the model produces, or based
        on what the model does. It turns out that even though there are so many
        different models, there are only three canonical tasks. These are:
      </p>
      <ul>
        <li>Classification</li>
        <li>Multi-Classification</li>
        <li>Regression</li>
      </ul>
      <p>
        Every deep learning model in the whole world can be broken down into
        these three tasks. And each of these tasks comes with a set of 'default'
        loss functions too. So if you have a never-before-solved problem, the
        first thing to do is figure out which task your problem falls under, and
        then pick a corresponding loss, and you've already solved half the issue
        in front of you.
      </p>
      <p>
        Let's talk about what each of these Canonical Tasks mean in a bit more
        detail.
      </p>

      <h6>Classification</h6>
      <p>
        A classification task is when you train a model to predict whether an
        input is one of a set of mutually exclusive things. Imagine we want to
        train a pet classifier. We want to pass the model some input, maybe an
        image. On the output, the model needs to decide whether the input
        represented a dog, a cat, or a fish. The model cannot say the input was
        a dog AND a cat AND a fish -- these are mutually exclusive categories.
        If the model is more confident that the input is a cat, it has to be
        less confident that the input is a dog or a fish.
      </p>
      <div class="image-holder">
        <img src="img/classification.jpg" width="400" />
        <p>An ML model learning a classification task.</p>
      </div>
      <p>
        What kind of loss would we use for classification? Well, as we said in
        <a href="simpledl_4.html">Part 4</a>, we care about three things: what
        the model produces, what we are comparing against, and how we do the
        comparison.
      </p>
      <p>
        Going back to our pet example, we want the model to produce a prediction
        for three categories. And, these three categories are totally unrelated
        from each other, except that they are mutually exclusive. So we can say
        that the model outputs a vector of size three (or an embedding layer of
        size three), and as long as we are consistent we can arbitrarily assign
        each spot in the vector to an animal -- so, spot 0 is 'dog', spot 1 is
        'cat', spot 2 is 'fish'. If the model puts a really high value in spot
        0, it's predicting dog.
      </p>
      <p>
        One problem. Right now, there's nothing stopping the model from putting
        super high weights on more than one value! What if the model outputs
        something like [1, 1, 1] -- it thinks the input image is a dog and a cat
        and a fish. Since we know our inputs are mutually exclusive, we can
        force the model to behave by constraining the inputs relative to each
        other. Softmax to the rescue! The softmax function takes in an input
        vector and reweights it so that the total value of all of the parts of
        the vector sum to one. Since we constrain the sum, the model has to make
        a choice -- if it puts more weight on one part of the vector, it has to
        put less weight on other parts of the vector. And, because it sums to 1,
        we can think of the output of the softmax as a probability distribution.
      </p>
      <p>
        A softmax also gives us a pretty natural way to represent our ground
        truth. We can compare the model's outputs against a 'perfect' output.
        So, if we input a picture of a cat, we expect the model to output
        something like [0, 1, 0]. Or if we put in a picture of a dog, we want
        the model to output [1, 0, 0].
      </p>
      <p>
        Ok, so we know what the model outputs, and what we compare against. How
        do we do the comparison? We can use something called a cross entropy
        loss. Explaining cross entropy is kinda out of scope for this piece, and
        also kinda irrelevant. Think of it as a black box that measures the
        difference between two probability distributions. Take a look at the
        [wiki page](https://en.wikipedia.org/wiki/Cross_entropy) for more.
      </p>
      <div class="image-holder">
        <img src="" width="400" />
      </div>
      <p>
        This setup is so common for classification tasks that it's basically the
        default. Every stats package worth its salt has a function called
        softmax cross entropy, that calculates the loss above.
      </p>

      <h6>Multi-Classification</h6>
      <p>
        A <b>multi-classification</b> task is similar to a classification task,
        except this time the possible outputs are not mutually exclusive. Maybe
        we have a dataset of zoo animals, and so it's possible for an image to
        contain cats and dogs and fish. We would use a multi-classification
        model to represent this data.
      </p>
      <p>
        A pretty common multi-classification loss is sigmoid cross entropy. This
        loss is similar to the softmax cross entropy, but we use a sigmoid
        nonlinearity because that is applied per element and simply converts
        each individual element to a value between 0 and 1. Because the
        individual elements aren't constrained by each other, multiple elements
        can be set to 1 (i.e. 'predicted'). One way to think about this model
        output is to imagine that each individual element is its own predictive
        function, and any value above a threshold (commonly 0.5) is therefore
        considered a model prediction.
      </p>

      <h6>Regression</h6>
      <p>
        A <b>regression</b> task is when you train a model to predict continuous
        values. The other two types of tasks predict categories that don't have
        any directional relation to each other. By comparison, regression tasks
        produce outputs where predicting a higher or lower value matters because
        of the numerical relationship between those values. For example, if we
        wanted to predict housing prices, or case counts for some disease, or
        some embedding value from a different model, we want to use regression.
      </p>
      <p>
        There are a bunch of possible losses here, such as difference, mean
        squared error or mean squared log error. There are theoretical reasons
        to prefer a log error, but basically as long as you have some measure of
        distance between the ground truth and the model prediction as your loss,
        you are probably good.
      </p>

      <h6>Outputs vs. Tasks</h6>
      <p>
        The tasks above aren't necessarily the same thing as the model output.
        These tasks basically define ways for us to think about and categorize
        models (or subsets of models). The actual outputs can be anything within
        the model. For example, we may want to train a model with a
        multiclassification loss on images, in order to pull out the top layer
        of the model as an image embedding. For your own intuition, it's
        important to avoid confusing these two concepts (even though they are
        fairly similar in many learning tasks).
      </p>
    </div>
  </div>
  <div class="footer">
    <a href="./index.html">Back to Writing</a>
  </div>
</div>
