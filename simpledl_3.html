<title>writing</title>
<meta name="viewport" content="width=800" />
<link rel="icon" href="spy.ico" type="image/x-icon" />
<link rel="shortcut icon" href="spy.ico" type="image/x-icon" />
<link rel="stylesheet" href="theme.css" />

<style></style>
<!-- Google Analytics -->
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-131666667-1"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());

  gtag("config", "UA-131666667-1");
</script>

<div class="header">
  <h1>Writing</h1>
  <h3>Amol Kapoor</h3>
</div>

<div class="content">
  <div class="writing-holder">
    <h4>Simple DL Part 3: Losses</h4>
    <h6>December, 2020</h6>

    <div class="writing">
      <h6>Recap</h6>
      <p>
        In the <a href="./simple2l_2.html">last section</a> we talked
        extensively about embeddings: what they are, how they worked, and why
        they are important. An embedding as a list of floats that carries some
        information about a concept; because embeddings are continuous
        representations, they can turn concepts into computation. A set of
        embeddings can be thought of as points on a map, that somehow represents
        the underlying 'important' structure of the things we are learning
        about. Loss functions are used to manipulate embeddings in a deep neural
        network -- in this section, we look at loss functions in more depth.
      </p>
      <h6>TLDR</h6>
      <ul>
        <li>
          Loss functions are how we tell the model what information to emphasize
          or to throw away.
        </li>
        <li>
          If an embedding is a map, the loss is the cartographer.
        </li>
        <li>
          When thinking about losses, we care about three things: what the model
          produces, what we are comparing against, and how we do the comparison.
        </li>
        <li>
          Losses can be placed on any embedding -- they do not just have to be
          at the end. The closer a loss is to an embedding, the more influence
          it will have.
        </li>
      </ul>
      <h6>What is a loss function?</h6>
      <p>
        A deep neural network takes in some piece of data represented as an
        embedding (i.e. a list of numbers). It passes the data through a stack
        of transformations, each of which produces an intermediate embedding.
        And then the model outputs some final embedding, depending on what the
        task is.
      </p>
      <div class="image-holder">
        <img src="img/turtles.jpg" width="400" />
        <p>
          It's embeddings all the way down.
        </p>
      </div>
      <p>
        During training, we want to quantify how good that final output is -- we
        do this with a loss function. In practice, the loss function takes the
        output of the DNN and produces a single number. The DNN tries to
        minimize that number. We're not gonna dive into <i>how</i> the model
        minimizes the loss, because I think in practice it's rarely relevant for
        understanding DNNs. But if you're interested take a look at the wiki
        page for
        <a href="https://en.wikipedia.org/wiki/Backpropagation">backprop</a>.
      </p>
      <p>
        If we want to try and put this into a single sentence, a loss function
        is a function f(x) or f(x, y) that measures how well the model output x
        matches a specific piece of information, possibly defined by some
        external source y. When thinking about a whole training set, the loss
        function is a measure of how well the model output matches a certain
        distribution.
      </p>
      <div class="image-holder">
        <img src="" width="400" />
        <p>
          TODO: Get loss distribution image.
        </p>
      </div>
      <h6>How do we change the loss function?</h6>
      <p>
        It's hard to figure out exactly how changing the loss will impact the
        model -- I mean, we can calculate the outcomes with math but
        realistically that does not provide us with a great understanding of how
        the model will respond. As a result, changing the losses of a model is
        pretty volatile. To a first approximation, we can tune our model by
        thinking about how the data is correlated with the outcomes we want, and
        adding losses that emphasize that data.
      </p>
      <p>
        There's already an example of this in the embeddings post; I'll try and
        use a different example here.
      </p>
    </div>
  </div>
  <div class="footer"></div>
</div>
