<title>SimpleDL Part 6: An End to End Example</title>
<meta name="viewport" content="width=800" />

<meta name="description" content="Building intuition about deep learning." />
<meta property="og:title" content="SimpleDL pt 6: An End to End Example" />
<meta property="og:type" content="website" />
<meta
  property="og:description"
  content="Building intuition about deep learning."
/>
<meta
  property="og:image"
  content="https://amolkapoor.com/writing/img/networkeffects.jpg"
/>
<meta name="twitter:title" content="SimpleDL pt 6: An End to End Example" />
<meta
  name="twitter:description"
  content="Building intuition about deep learning."
/>
<meta
  name="twitter:image"
  content="https://amolkapoor.com/writing/img/networkeffects.jpg"
/>
<meta name="twitter:card" content="summary_large_image" />

<link rel="icon" href="spy.ico" type="image/x-icon" />
<link rel="shortcut icon" href="spy.ico" type="image/x-icon" />
<link rel="stylesheet" href="theme.css" />

<!-- Syntax highlighting -->
<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/default.min.css"
/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>

<!-- Google Analytics -->
<script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-131666667-1"
></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag('js', new Date());

  gtag('config', 'UA-131666667-1');
</script>

<div class="header">
  <h1>Writing</h1>
  <h3>Amol Kapoor</h3>
</div>

<div class="content">
  <div class="writing-holder">
    <h4>Simple DL Part 6: An End to End Example (with Code!)</h4>
    <h6>June, 2021</h6>

    <div class="writing">
      <h6>TLDR</h6>
      <ul>
        <li></li>
      </ul>

      <h6>Overview</h6>
      <p>
        At this point, you hopefully have a high level understanding of some
        key deep learning principles: features, embeddings, losses, and how
        they all interact with each other. Unfortunately for me, some of my
        readers complained that this was not enough, and that without an end to
        end example that showed how the intuition could be applied, this whole
        project was meaningless. <b>sigh</b>. Even though I was really hoping
        to avoid digging into the specifics of a deep learning library, I think
        my readers are probably right. Hold on tight folks, this one is going
        to be a long one.
      </p>
      <p>
        There are
        <a href="https://nextjournal.com/gkoehler/pytorch-mnist">countless</a>
        <a href="https://www.tensorflow.org/tutorials/quickstart/beginner"
          >starter</a
        >
        <a href="https://roberttlange.github.io/posts/2020/03/blog-post-10/"
          >examples</a
        >
        for deep learning. Most of these implement a small neural network that
        can learn to classify handwritten numbers from 0 to 9 (aka
        <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>). Many
        of these tutorials are quite good for understanding the particular
        syntax of a specific library, but they do a poor job of linking the
        code to some deeper understanding of ML. In part that is because the
        deeper understanding doesn't really exist -- the answer to 'why' is
        'because'.
      </p>
      <div class="image-holder">
        <img src="img/Because.png" width="500" />
        <p>
          I wish I could say that this frustration goes away. It doesn't. I
          still feel like this when I read new ML papers.
        </p>
      </div>
      <p>
        In this tutorial, I'll instead do something totally different by
        teaching you how to implement a small neural network that can learn to
        classify handwritten numbers from 0 to 9. We're not going to touch code
        until the very end -- instead, we'll spend a lot of time trying to
        think through the problem in order to build some intuition of what we
        should be doing. All parts of the tutorial will be grounded in the
        previous SimpleDL lessons. Even though MNIST is a really well known
        dataset with countless 'solutions', I'll try to approach the task as if
        it was a real world learning problem.
      </p>
      <h6>The Problem</h6>
      <p>
        You work for the IRS. You have to deal with millions of tax filings --
        over 150M, according to a random website called Google. That's a ton of
        filings. Most people do these on printed forms, filling in fields by
        hand. The techs over at the Department of Technology have scanned all
        the filings. Now they need to pull out all of the numbers.
      </p>
      <p>One problem: the scans are all unparsed images.</p>
      <p>
        We need to build a system that can convert images of numbers into
        actual numbers in some programming language or database, so that we can
        do more number crunching down the line. Unfortunately, there are tons
        of edge cases, which makes most statistical/geometric/traditional
        computer vision approaches obsolete. A human can probably figure out
        most of them, but humans are expensive and slow. Can we use deep
        learning?
      </p>
      <div class="image-holder">
        <img src="img/No-image-found.jpg" width="300" />
        <p>
          Pictured: the US Federal Department of Technology logo (the joke is
          that there's no such thing).
        </p>
      </div>

      <h6>Using Canonical Tasks: Loss</h6>
      <p>
        In <a href="./simpledl_5.html">Part 5</a>, we laid out three canonical
        tasks: classification, multiclassification, and regression. If we
        figure out which bucket our IRS problem falls in, we can infer a
        default loss for our model. Let's work through each possibility in
        reverse, starting with regression.
      </p>
      <p>
        A regression task is one where we try to predict a continuous output.
        It is tempting to look at the IRS problem and say that since we are
        predicting numeric output, the problem space must be continuous.
        Unfortunately, that way lies madness. The trick is that even though we
        are predicting numbers, we are treating each number as a discrete
        category. In other words, if the true label is '1', the model is
        equally wrong if it predicts '1' or '9'. If the ordering output doesn't
        matter, it's not a regression problem.
      </p>
      <p>
        What about multiclassification? The main difference between
        classification and multiclassification is whether the categories are
        mutually exclusive. In our IRS problem, each input could only be one of
        ten digits (0 - 9). In other words, they ARE mutually exclusive.
      </p>
      <p>
        That leaves classification as the only remaining option. In Part 5, we
        showed that the Softmax Cross Entropy is a standard loss for
        classification tasks. Following the example, we can label our training
        data with a one-hot vector of size 10, where the hot index is the true
        number. We make the model output a vector (logits) of size 10, so we
        can compare the output with the ground truth labels. And then we just
        pass these into
        <a
          href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"
          >the appropriate function</a
        >
        and call it a day.
      </p>
      <p>If this all made sense, congrats. This is the hardest part.</p>
      <h6>What about the Features?</h6>
      <p>
        Some people think feature selection is a fine art. Those people are
        wrong. In deep learning, feature selection is something of a misnomer
        -- if you have features available, you should use them. The problem is
        that features are really hard to get, because they have to be
        consistent across all of the training and test data.
      </p>
      <p>
        In our IRS problem, we can't be sure we have any metadata available
        consistently. People forget to add names, or addresses, or whatever. In
        fact, the only thing we <i>can</i> be sure of is that we have the raw
        pixels of the number we're trying to predict. So...let's just use that
        as our features.
      </p>
      <p>
        But of course we can't just pass an image directly into a model from a
        filepath (ignoring
        <a
          href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img"
          >this</a
        >). You have to convert all features into a numeric vector
        representation first. Luckily, for images this is really easy -- each
        pixel is already a numeric RGB value. We can convert the images to
        numbers using a
        <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html"
          >python library of choice</a
        >
        and then use that as input. Each image ends up being a 3D input, with
        shape [Batch, Height, Width , Channels]. If we want to feed multiple
        images in at once, we can stack the vectors on a new axis, resulting in
        a [Batch, Height, Width, Channels]. O, and you are going to want to
        <a href="https://en.wikipedia.org/wiki/Feature_scaling">normalize</a>
        the features, but that's a conversation for later.
      </p>
      <h6>What about the Model?</h6>
      <p>
        At this point, we know the input (image matrices) and the output (a
        logits vector of size 10). So we can slot in just about any model we
        want, as long as it constrains to the input and output.
      </p>
      <p>
        Does the model choice matter? Well, kinda. Models have a certain
        'capacity' that limits what kind of problems it can effectively solve.
        The problem is, we haven't really figured out how to calculate or
        define 'capacity'. Most engineers use the number of parameters in the
        model as a rough heuristic. More parameters = more capacity. On some
        level this is intuitive, but we also recognize that a multi-layer model
        is better than a flat model, even if they have the same parameter size.
      </p>
      <p>I digress.</p>
      <p>
        As long as we choose a model that isn't literally a single flat layer,
        we should be alright. In the industry, we broadly slice models by task
        -- convolutional networks for image/video processing, transformers for
        language processing, graph neural networks for graphs, who-knows-what
        for audio, etc. Since we are using images, we can use a convolutional
        network.
      </p>
      <h6>What about...everything else?</h6>
      <p>
        It is a little reductive to say that the above is all you need -- in
        practice, it's about 95% of what you need. There are two pieces left.
      </p>
      <p>
        One piece that we're missing is called an optimizer. Optimizers are
        algorithms that calculate how to turn your model's output loss into
        gradients for each part of the model. Generally, you can set a learning
        rate and a few other parameters that determine how much each step of
        training impacts the model. A deep dive into how optimizers work isn't
        really in scope here. Suffice to say, there's a lot of interesting
        kinds of optimizers out there, and they all do slightly different
        things, and you pretty much always want to use the AdamOptimizer with a
        learning rate of 0.0001.
      </p>
      <div class="image-holder">
        <img src="" width="500" />
        <p>This is Adam. He optimizes things.</p>
      </div>
      <p>
        The other piece of the puzzle is our batching algorithm. Ideally, we
        could feed our model every piece of data in the dataset in one go.
        Unfortunately, for really large datasets, we don't have the computer
        memory to do that. So we instead sample the dataset into 'batches' and
        train the model one batch at a time. When we go through the entire
        dataset, we say that the model has trained for one 'epoch'.
      </p>
      <div class="image-holder">
        <img src="" width="500" />
        <p>One epoch is many batches.</p>
      </div>
      <p>
        A deep dive into how batching works isn't really in scope here either.
        Suffice to say, there are a lot of batching algorithms, and a lot of
        people are doing really cool work understanding how sampling strategies
        impact model training, and you pretty much always want to use random
        sampling with the largest batch size you can.
      </p>
      <h6>Let's code!</h6>
      <pre><code class='language-python'>
def get_features(paths):
  pass

def get_model():
  pass

def get_loss(logits, labels):
  pass

def get_optimizer():
  pass

def batch(iterable, batch_size=1):
  """Given an iterable, produce batches of size batch_size."""
  for idx in range(0, len(iterable), batch_size):
    yield iterable[idx:min(idx + batch_size, len(iterable))]

if __name__ == '__main__':
  PATHS = glob.glob('path/to/images/*')
  EPOCHS = 10
  BATCH_SIZE = 64
  model = get_model()
  optimizer = get_optimizer()
  for epoch in range(EPOCHS):
    # randomize the paths for each epoch.
    for batch in batch(PATHS, BATCH_SIZE):
      data, labels = features(batch)
      logits = model(data)
      loss = get_loss(logits, labels)
      optimizer(model, loss)

    print(f'Loss at epoch {epoch} is {loss}')

  INFER_PATHS = glob.glob('path/to/inference/images/*')
  data, labels = features(INFER_PATHS)
  logits = model(data)
  predictions = np.argmax(logits)
  accuracy = get_accuracy(predictions, labels)
  print('Model accuracy: ', predictions)

      </code></pre>
      <h6>Conclusions</h6>
      <p>
        Deep learning isn't really software engineering. A software engineer
        spends time thinking about data structures, and interfaces, and
        abstractions, and representing all of these things with code. In
        contrast, the little bit of code that deep learning engineers actually
        write is more like setting up scaffolding. It's not that hard to write
        any specific part of a deep learning pipeline; the hard part is making
        sure your data pipeline does what you want.
      </p>
      <p>
        This is why people use MNIST as an introduction, and why experts will
        use MNIST to debug their models when things go wrong. It's pretty easy
        to set up an MNIST data pipeline because the data is in a format that
        is very easy to use. All the images are the same size, in the same
        format, with accurate labels. The entire dataset is small enough that
        you can actually iterate in a reasonable timeframe. And a simple MNIST
        solution accurately demonstrates the rough skeleton for tackling deep
        learning: features, model, loss. Figure out how to turn your data into
        an embedding that makes sense for your specific problem, and you're
        good to go.
      </p>
    </div>
  </div>
  <div class="footer">
    <a href="./index.html">Back to Writing</a>
  </div>
</div>

<script>
  hljs.highlightAll();
</script>
